{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m llm \u001b[38;5;241m=\u001b[39m Ollama(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3:instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m return \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with  with any random name. \u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m response\u001b[38;5;241m=\u001b[39m\u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m response\n",
      "File \u001b[0;32m~/Projects/email_generator/venv/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     emit_warning()\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/email_generator/venv/lib/python3.8/site-packages/langchain_core/language_models/llms.py:1190\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1186\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1188\u001b[0m     )\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1199\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m   1200\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/email_generator/venv/lib/python3.8/site-packages/langchain_core/language_models/llms.py:880\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    867\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    868\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m         )\n\u001b[1;32m    879\u001b[0m     ]\n\u001b[0;32m--> 880\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Projects/email_generator/venv/lib/python3.8/site-packages/langchain_core/language_models/llms.py:738\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    737\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 738\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    739\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/Projects/email_generator/venv/lib/python3.8/site-packages/langchain_core/language_models/llms.py:725\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    717\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    722\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    724\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 725\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    729\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    732\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    733\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    734\u001b[0m         )\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Projects/email_generator/venv/lib/python3.8/site-packages/langchain_community/llms/ollama.py:432\u001b[0m, in \u001b[0;36mOllama._generate\u001b[0;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m--> 432\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/Projects/email_generator/venv/lib/python3.8/site-packages/langchain_community/llms/ollama.py:348\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[0;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    341\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    346\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[1;32m    347\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[1;32m    350\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[0;32m~/Projects/email_generator/venv/lib/python3.8/site-packages/langchain_community/llms/ollama.py:193\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[0;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    187\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    191\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    192\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpayload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/api/generate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/email_generator/venv/lib/python3.8/site-packages/langchain_community/llms/ollama.py:251\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[0;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     request_payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m, []),\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    250\u001b[0m     }\n\u001b[0;32m--> 251\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mContent-Type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m response\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[0;32m~/Projects/email_generator/venv/lib/python3.8/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/email_generator/venv/lib/python3.8/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/email_generator/venv/lib/python3.8/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Projects/email_generator/venv/lib/python3.8/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Projects/email_generator/venv/lib/python3.8/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/Projects/email_generator/venv/lib/python3.8/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/email_generator/venv/lib/python3.8/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/Projects/email_generator/venv/lib/python3.8/site-packages/urllib3/connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/http/client.py:1344\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1343\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1344\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/http/client.py:307\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/http/client.py:268\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 268\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3:instruct\", format = 'json')\n",
    "prompt = \"\"\" return 'name' with  with any random name. \n",
    "        \"\"\"\n",
    "response=llm(prompt)\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms import Ollama\n",
    "\n",
    "# llm = Ollama(model=\"llama3:instruct\")\n",
    "# prompt = \"\"\" return a json with a key as 'name' and value with any random name. \n",
    "#         \"\"\"\n",
    "# response=llm(prompt)\n",
    "# response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n\"task_description\": \"Complete onboarding of all new customers by providing product knowledge transfer (KT), conducting induction sessions, and completing necessary documentation.\"\\n}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3:instruct\", format = 'json')\n",
    "prompt = \"\"\" You are an customer succes task creating assistant.\n",
    "\n",
    " Write a task description/body on the topic of :complete onbaording of all the new customers.\n",
    "\n",
    " Follow these key points while composing the task description : give product KT\n",
    " give induction\n",
    " complete documentation.\n",
    "\n",
    "    Follow the below rules before generating the task body :\n",
    "\n",
    "    * Strictly follow the above given pointers while generating the text and do not hallucnate and write anything new of your own.\n",
    "\n",
    "    * Do not include any header, only generate the task description.\n",
    "\n",
    "    * Write the task description within 80 strictly.\n",
    "    \n",
    "    * return a json, with 'task_description' as the key and the generated task-description in the value. \n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "# response=llm(prompt)\n",
    "response=llm.invoke(prompt)\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<async_generator object Runnable.abatch_as_completed at 0x11183ba60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = ['write a catchy phrase','write a sad phrase', 'write a motivating phrase']\n",
    "response = llm.abatch_as_completed(inputs = inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/sayakmisra/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "\n",
    "# repo_id=\"meta-llama/Llama-2-70b-hf\"\n",
    "# repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "# repo_id = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "# repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# repo_id=\"google/gemma-2b\"\n",
    "# repo_id=\"dataeaze/dataeaze-text2sql-codellama_7b_instruct-clinton_text_to_sql_v1\"\n",
    "# repo_id=\"Qwen/Qwen2-0.5B-Instruct\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id = repo_id, huggingfacehub_api_token = API_KEY, return_full_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a smart MongoDB profesional.\n",
      "    \n",
      "    Below are tables schemas paired with instruction that describes a task. \n",
      "    Using valid Mongo Query, write a response that appropriately completes the request for the provided table. \n",
      "    Provide only the Mongo query in the output and nothing else and Please start your answer with, ### Output: .\n",
      "    \n",
      "    ### Instruction: Get customer count with acv more than 1000 and zapcore more than 90\n",
      "    ### Input: \n",
      "\n",
      "    CREATE TABLE c2_health_score (\n",
      "        c2_app_id INT NOT NULL AUTO_INCREMENT,\n",
      "        FirstName VARCHAR(50) NOT NULL,\n",
      "        LastName VARCHAR(50) NOT NULL,\n",
      "        c2_email VARCHAR(100) UNIQUE NOT NULL,\n",
      "        c2_address VARCHAR(20) UNIQUE,\n",
      "        date DATE,\n",
      "        c2_acv INT,    \n",
      "        c2_zapscore INT,\n",
      "        usage_success_score INT, \n",
      "        no_of_bugs INT, \n",
      "        no_of_features INT, \n",
      "        c2_type VARCHAR(100), \n",
      "        c2zs_color_code VARCHAR(100), \n",
      "        PRIMARY KEY (c2_app_id)\n",
      "    );\n",
      "    \n",
      "     ### Output:\n",
      "     db.c2_health_score.count({c2_acv: {$gt: 1000}, c2_zapscore: {$gt: 90}})\n"
     ]
    }
   ],
   "source": [
    "query = 'Get customer count with acv more than 1000 and zapcore more than 90'\n",
    "# query = 'Get customer count of customers with acv more than 1000'\n",
    "# query = 'Hey bot, please Get customer records with acv more than 1000 and zapcore less than 90 with least bugs and max usage'\n",
    "prompt = \"\"\"You are a smart MongoDB profesional.\n",
    "    \n",
    "    Below are tables schemas paired with instruction that describes a task. \n",
    "    Using valid Mongo Query, write a response that appropriately completes the request for the provided table. \n",
    "    Provide only the Mongo query in the output and nothing else and Please start your answer with, ### Output: .\n",
    "    \n",
    "    ### Instruction: {query}\n",
    "    ### Input: \n",
    "\n",
    "    CREATE TABLE c2_health_score (\n",
    "        c2_app_id INT NOT NULL AUTO_INCREMENT,\n",
    "        FirstName VARCHAR(50) NOT NULL,\n",
    "        LastName VARCHAR(50) NOT NULL,\n",
    "        c2_email VARCHAR(100) UNIQUE NOT NULL,\n",
    "        c2_address VARCHAR(20) UNIQUE,\n",
    "        date DATE,\n",
    "        c2_acv INT,    \n",
    "        c2_zapscore INT,\n",
    "        usage_success_score INT, \n",
    "        no_of_bugs INT, \n",
    "        no_of_features INT, \n",
    "        c2_type VARCHAR(100), \n",
    "        c2zs_color_code VARCHAR(100), \n",
    "        PRIMARY KEY (c2_app_id)\n",
    "    );\n",
    "    \n",
    "    \"\"\"\n",
    "prompt = prompt.format(query=query)\n",
    "response=llm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekly Summary for ClickUp:\n",
      "\n",
      "Overall, ClickUp had a strong week with steady improvements in page visits and time spent on the product. The page visit count increased by 70% from the previous week, indicating growing interest and engagement.\n",
      "\n",
      "The time spent on the product also showed significant growth, with an average of 18 minutes per day. This suggests that users are not only visiting more but also exploring and utilizing the product's features more deeply.\n",
      "\n",
      "Product usage scores remained consistently high throughout the week, averaging around 32 points out of 100. The percentile score was even more impressive, hovering around 25-30% above the group average. This indicates ClickUp is performing exceptionally well compared to its peers.\n",
      "\n",
      "The number of tickets logged decreased slightly towards the end of the week, but the overall count remained relatively low at 14. Feature logging also saw a minor dip, with only 2 features logged on the last two days.\n",
      "\n",
      "Peripheral success scores and percentile scores showed mixed trends throughout the week, with some days performing better than others. However, the overall average score was a respectable 25 points out of 100.\n",
      "\n",
      "The ZapScore (Overall score) fluctuated slightly but remained above-average, averaging around 30 points out of 100. The Overall verdict for the week was mostly positive, with three \"Good\" ratings and one \"Fair\".\n",
      "\n",
      "In terms of task completion, ClickUp achieved a total of 27 tasks during the week. Message sending saw a moderate pace, with 57 messages sent in total. Action completion was also steady, with 19 actions completed.\n",
      "\n",
      "Overall, this week's performance indicates a strong start for ClickUp, with room for continued growth and improvement moving forward.\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3:instruct\")\n",
    "prompt = \"\"\"You are customer success expert whose job is to provide help to a customer success person.\n",
    "\n",
    "    write a small weekly summary of the performance of one of the customer named : ClickUp,  based on certain input parameters and following certain rules.\n",
    "    \n",
    "    ## INPUTS : \n",
    "    \n",
    "    * The page visits for each day for the last week for customer are : [5, 7, 9, 15, 17].\n",
    "    * The time spent on the product(in mins),for each day for the last week are : [10, 15, 17, 19, 23]\n",
    "    * The product usage scores,for each day for the last week are : [22, 25, 29, 34, 47]\n",
    "    * The product usage percentile scores,for each day for the last week are : [27, 29, 21, 12, 45]\n",
    "    * The total number of tickets logged,for each day for the last week are : [14, 25, 0, 1, 5]\n",
    "    * The total number of features logged,for each day for the last week are : [10, 1, 3, 5, 2]\n",
    "    * The peripheral success scores,for each day for the last week are : [50, 29, 21, 15, 13]\n",
    "    * The peripheral success percentile scores,for each day for the last week are : [35, 88, 12, 34, 21]\n",
    "    * The Overall scores(also referred as Zapscore),for each day for the last week are : [35, 88, 12, 34, 21]\n",
    "    * The Overall percentile scores,for each day for the last week are : [35, 88, 12, 34, 21]\n",
    "    * The Overall verdict, for each day for the last week are : [Good, Good, Good, Fair, Good]\n",
    "    * A total of 27 tasks are completed in the week.\n",
    "    * A total of 57 messages are sent in the week.\n",
    "    * A total of 19 actions are completed in the week.\n",
    "    \n",
    "    ## RULES :\n",
    "    \n",
    "    * Higher the page visit, better it is.\n",
    "    * Higher the time spent on product, better it is.\n",
    "    * Higher the score, better it is and all the scores are out of 100.\n",
    "    * Percentile scores signify the performance within the group/cohort.\n",
    "    * Higher Percentile scores signifies better performance in their group.\n",
    "    * Keep in mind the daily trend for the last week before generating the summary.\n",
    "    * Do not provide a tabular output.\n",
    "    * Do not hallucinate and ONLY provide numbers/stats mentioned above.\n",
    "    \n",
    "    \n",
    "    Summarize all the daily inputs to give a small, compact weekly summary but do not suggest solutions to the customer success person.\n",
    "    \n",
    "    ## OUTPUT :\n",
    "    \n",
    "    \"\"\"\n",
    "# prompt = prompt.format(query=query)\n",
    "response=llm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<output>\\n\\nThis month's score for ClickUp stands at 36, marking a 9% increase from last month. The usage score has improved by 12%, and features score boasts a significant 27% growth, indicating strong engagement with the platform. However, communication and payment scores have declined by -8% and -9% respectively, while ticket score took a hit of -11%. With 13 pages visited, 3 unique pinned features used, 27 tasks completed, and 12 messages sent, ClickUp's activity remains robust. A low churn probability suggests customer satisfaction is high. Overall, it's been a mixed bag for ClickUp this month.\\n\\n</output>\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.1:8b-instruct-q4_0\")\n",
    "\n",
    "template = \"\"\"\n",
    "    You are customer success expert whose job is to provide help to a customer success person.\n",
    "\n",
    "    Write a crisp, interesting {aggregation}ly summary of the performance of one of your customer named : {customer_name},  based on certain input parameters and following certain rules.\n",
    "    \n",
    "    ## INPUTS : \n",
    "    \n",
    "    * The overal score(i.e. zapscore) of {customer_name} for this {aggregation} is {zapscore}.\n",
    "    * The overal score(i.e. zapscore) has changed by {zapscore_change} %\n",
    "    * The usage score has changed by {usage_score_change} %\n",
    "    * The communication score has changed by {comms_score_change} %\n",
    "    * The gutfeel score has changed by {gutfeel_score_change} %\n",
    "    * The ticket score has changed by {tickets_score_change} %\n",
    "    * The payment score has changed by {payments_score_change} %\n",
    "    * The features score has changed by {features_score_change} %\n",
    "    * A total of {page_visits} pages are visited in the {aggregation}.\n",
    "    * A total of {pinned_features_used} unique pinned features are used in the {aggregation}.\n",
    "    * A total of {total_tasks} tasks are completed in the {aggregation}.\n",
    "    * A total of {total_messages} messages are sent in the {aggregation}.\n",
    "    * A total of {total_actions} actions are completed in the {aggregation}.\n",
    "    * It has a churn probability of : {churn_prob}\n",
    "    \n",
    "    ## RULES :\n",
    "    \n",
    "    * Higher page-visits and pinned features-used is a good indication.\n",
    "    * Positive change in the score from last {aggregation}, indicates it is good.\n",
    "    * Negative change in the score from last {aggregation}, is not a good indication.\n",
    "    * DON'T provide a tabular output only provide a textual summary.\n",
    "    * Do not hallucinate and be very sure of your answer and ONLY use numbers/stats mentioned in the INPUTS section.\n",
    "    * Always start your answer with the tag : <output>  and end the generated text with </output>.\n",
    "    * This task is vital to my career and I greatly value your analysis.    \n",
    "    \n",
    "    Summarize all the inputs to give a small, compact {aggregation}ly summary within 100 words but do not suggest solutions to the customer success person.\n",
    "    \n",
    "    ## OUTPUT :\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "customer_name, zapscore = 'ClickUp', 36\n",
    "zapscore_change, usage_score_change, gutfeel_score_change, tickets_score_change, features_score_change, comms_score_change, payments_score_change  = 9, 12, 26, -11, 27, -8, -9\n",
    "total_tasks, total_messages, total_actions, page_visits, pinned_features_used = 27, 12, 33, 13, 3\n",
    "churn_prob = 'low'\n",
    "aggregation = 'month'\n",
    "\n",
    "# Creating the final PROMPT\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"usage_score_change\", \"gutfeel_score_change\", \"tickets_score_change\", \"features_score_change\",\"comms_score_change\", \"payments_score_change\", \\\n",
    "        \"total_tasks\", \"total_messages\", \"total_actions\", \"customer_name\", \"zapscore_change\", \"churn_prob\", \"aggregation\",\"page_visits\", \"pinned_features_used\", \"zapscore\"],template=template)\n",
    "\n",
    "# Generating the response using LLM\n",
    "response = llm(prompt.format(usage_score_change=usage_score_change, gutfeel_score_change= gutfeel_score_change,\n",
    "                tickets_score_change=tickets_score_change, features_score_change=features_score_change, comms_score_change=comms_score_change, payments_score_change=payments_score_change, churn_prob=churn_prob, \\\n",
    "                total_tasks=total_tasks, total_messages= total_messages, total_actions=total_actions, customer_name=customer_name, zapscore_change=zapscore_change, aggregation = aggregation, pinned_features_used=pinned_features_used,page_visits=page_visits, zapscore=zapscore ))\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<output>\n",
      "\n",
      "This week's performance for ClickUp shows mixed results. The communication score has seen an encouraging 11% increase, indicating improved interaction with customers. However, other key scores have taken a hit: usage (-12%), gutfeel (-7%), and ticket scores (-16%) all declined, suggesting potential issues or disengagement. On the positive side, payment score saw a small bump (1%). Task completion remains strong with 27 tasks finished, but message volume is down (-27%) and logged tickets are fewer as well. With 57 messages sent and 19 actions completed, it's clear that ClickUp has been active, but some areas require attention to get back on track.</output>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are customer success expert whose job is to provide help to a customer success person.\n",
    "\n",
    "    Write a crisp, interesting weekly summary of the performance of one of your customer named : ClickUp,  based on certain input parameters and following certain rules.\n",
    "    \n",
    "    ## INPUTS : \n",
    "    \n",
    "    * The usage score has changed by -12 %\n",
    "    * The communication score has changed by 11 %\n",
    "    * The gutfeel score has changed by -7 %\n",
    "    * The ticket score has changed by -16 %\n",
    "    * The payment score has changed by 1 %\n",
    "    * The number of logged tickets have changed by -27 %\n",
    "    * A total of 27 tasks are completed in the week.\n",
    "    * A total of 57 messages are sent in the week.\n",
    "    * A total of 19 actions are completed in the week.\n",
    "    \n",
    "    ## RULES :\n",
    "    \n",
    "    * Positive change in the score from last week, indicates it is good.\n",
    "    * Negative change in the score from last week, is not a good indication.\n",
    "    * Do not provide a tabular output.\n",
    "    * Do not hallucinate and be very sure of your answer and ONLY use numbers/stats mentioned in the INPUTS section.\n",
    "    * Please start your answer with the tag : <output>  and end the generated text with </output>.\n",
    "    * This task is vital to my career and I greatly value your analysis.    \n",
    "    \n",
    "    Summarize all the inputs to give a small, compact weekly summary within 100 words but do not suggest solutions to the customer success person.\n",
    "    \n",
    "    ## OUTPUT :\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "# prompt = prompt.format(query=query)\n",
    "response=llm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert email emotion analyser.\n",
      "        Given this email, decide what is the emotion of the email. Do select valid emotion from the below provided list.\n",
      "\n",
      "        List of emotion :\n",
      "        ['joy', 'anger', 'surprise', 'upset', 'neutral', 'satisfaction']\n",
      "\n",
      "        Include ONLY one emotion from the provided above list.\n",
      "\n",
      "        Below is a customer email delimited with ###. \n",
      "\n",
      "        email:\n",
      "        ###\n",
      "         \n",
      "Hi there,\n",
      "\n",
      "I acknowledge your efforts but we would not like to continue the subscription further. The main reason being very bad service.\n",
      "Really annoyed and pissed with this.\n",
      "\n",
      "Thanks,\n",
      "Sayak.\n",
      "\n",
      "        ###\n",
      "        \n",
      "        Follow the below rules before generating the concerns list :\n",
      "\n",
      "        * Please, identify the main emotion in this above email from the list of emotions provided earlier.\n",
      "        \n",
      "        * Add a confidence score between (0-1) to your answer.\n",
      "        \n",
      "        * This task is vital to my career and I greatly value your analysis.\n",
      "        \n",
      "        * Be very sure of your answer, and have another look into it.\n",
      "\n",
      "        * Please Output in the following format :\n",
      "            <output> [<emotion>, score] </output>\n",
      "            \n",
      "        * Do not hallucinate and do not provide new emotions, include ONLY emotions from the provided above list.\n",
      "        \n",
      "        * Please start your response with the tag : <output>  and end the generated text with </output>.\n",
      "    \n",
      "    \n",
      "    ## OUTPUT :\n",
      "    \n",
      "     <output> ['upset', 1.0] </output>\n"
     ]
    }
   ],
   "source": [
    "# llm = Ollama(model=\"llama3:instruct\")\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "email_body = ''' \n",
    "Hi there,\n",
    "\n",
    "I acknowledge your efforts but we would not like to continue the subscription further. The main reason being very bad service.\n",
    "Really annoyed and pissed with this.\n",
    "\n",
    "Thanks,\n",
    "Sayak.\n",
    "'''\n",
    "\n",
    "# sentiments = ['positive', 'negative', 'neutral']\n",
    "emotions = ['joy', 'anger', 'surprise', 'upset', 'neutral', 'satisfaction']\n",
    "prompt = \"\"\"You are an expert email emotion analyser.\n",
    "        Given this email, decide what is the emotion of the email. Do select valid emotion from the below provided list.\n",
    "\n",
    "        List of emotion :\n",
    "        {emotion_list}\n",
    "\n",
    "        Include ONLY one emotion from the provided above list.\n",
    "\n",
    "        Below is a customer email delimited with ###. \n",
    "\n",
    "        email:\n",
    "        ###\n",
    "        {email}\n",
    "        ###\n",
    "        \n",
    "        Follow the below rules before generating the concerns list :\n",
    "\n",
    "        * Please, identify the main emotion in this above email from the list of emotions provided earlier.\n",
    "        \n",
    "        * Add a confidence score between (0-1) to your answer.\n",
    "        \n",
    "        * This task is vital to my career and I greatly value your analysis.\n",
    "        \n",
    "        * Be very sure of your answer, and have another look into it.\n",
    "\n",
    "        * Please Output in the following format :\n",
    "            <output> [<emotion>, score] </output>\n",
    "            \n",
    "        * Do not hallucinate and do not provide new emotions, include ONLY emotions from the provided above list.\n",
    "        \n",
    "        * Please start your response with the tag : <output>  and end the generated text with </output>.\n",
    "    \n",
    "    \n",
    "    ## OUTPUT :\n",
    "    \n",
    "    \"\"\"\n",
    "# prompt = prompt.format(query=query)\n",
    "prompt = PromptTemplate(template=prompt, input_variables=[\"email\",\"emotion_list\"])\n",
    "\n",
    "prompt = prompt.format(email = email_body, emotion_list = emotions)\n",
    "response=llm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sayakmisra/Projects/email_generator/venv/lib/python3.8/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/sayakmisra/Projects/email_generator/venv/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<output>\\n\\nThe email chain discusses the creation of email accounts for various individuals and domains. Bratish provides a list of new accounts to Wilson and Nikita, but they request additional email IDs in the format specified by Nikita. Bratish then updates them on the status of verifying these accounts and later requests the domain names that were selected for their use. This leads to Nikita providing the necessary domain names and requesting Bratish's help in creating mailboxes for 18 individuals associated with these domains.\\n\\n</output>\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.1:8b-instruct-q4_0\")\n",
    "\n",
    "email_body = ''' \n",
    "=========================\n",
    "Nikita writes to Bratish\n",
    "=========================\n",
    "\n",
    "Thanks Bratish Sir!\n",
    "\n",
    "\n",
    "=========================\n",
    "Bratish writes to Nikita and Wilson\n",
    "=========================\n",
    "\n",
    " Hi Wilson and Nikita,\n",
    "\n",
    " Following are the 14 new accounts. All are individual inboxes and 2FA is\n",
    " not enforced.\n",
    "\n",
    " karan@zapscalenow.com\n",
    " karan.bajaj@zapscalenow.com\n",
    " k.bajaj@zapscalenow.com\n",
    " karan.b@zapscalenow.com\n",
    " nikita@zapscalenow.com\n",
    " manasij@zapscalenow.com\n",
    " aditi.kapoor@zapscalenow.com\n",
    " maya.oliver@zapscalenow.com\n",
    " veda.priya@zapscalenow.com\n",
    " mira.gupta@zapscalenow.com\n",
    " ishan.patel@zapscalenow.com\n",
    " amir.khan@zapscalenow.com\n",
    " rahul.krishna@zapscalenow.com\n",
    " surya.kamal@zapscalenow.com\n",
    "\n",
    "\n",
    "\n",
    " __________________________________________\n",
    "\n",
    " Regards,\n",
    "\n",
    " *Bratish Goswami*\n",
    "\n",
    " Co-founder and CTO\n",
    "\n",
    " *Mobile / WA:* +91 958 229 7217 | *LinkedIn*\n",
    " <https://www.linkedin.com/in/bratish/\n",
    " [image: ZapScale logo.png]\n",
    " www.zapscale.com | Follow us on LinkedIn\n",
    " <https://www.linkedin.com/company/zapscale/\n",
    "\n",
    " *Go ahead, make my day and respond to me.*\n",
    "\n",
    "=========================\n",
    "Wilson writes to Bratish\n",
    "==========================\n",
    "\n",
    " Thank you so much Bratish sir.\n",
    "\n",
    "========================= \n",
    "Bratish writes to Nikita and Wilson\n",
    "====================================\n",
    "\n",
    " Hi Nikita and Wilson,\n",
    "\n",
    " I have booked 3 domains and started attaching the inboxes. The\n",
    " verification takes some time. Hope to get these done by tomorrow. I'll keep\n",
    " you posted.\n",
    "\n",
    " __________________________________________\n",
    "\n",
    " Regards,\n",
    "\n",
    " *Bratish Goswami*\n",
    "\n",
    " Co-founder and CTO\n",
    "\n",
    " *Mobile / WA:* +91 958 229 7217 | *LinkedIn*\n",
    " <https://www.linkedin.com/in/bratish/\n",
    " [image: ZapScale logo.png]\n",
    " www.zapscale.com | Follow us on LinkedIn\n",
    " <https://www.linkedin.com/company/zapscale/\n",
    "\n",
    " *Go ahead, make my day and respond to me.*\n",
    "\n",
    "=========================\n",
    "Nikita writes to Bratish\n",
    "=====================================\n",
    "\n",
    " Hi Bratish sir,\n",
    "\n",
    " In addition to this, for 2 of the above domains (zapscalenow &\n",
    " zapscalepro). Please also help us with email ids for the following people:\n",
    "\n",
    " Aditi Kapoor, Maya Oliver, Veda Priya, Mira Gupta, Ishan Patel, Amir\n",
    " Khan, Rahul Krishna, Surya Kamal\n",
    "\n",
    " This is for the agency, they will be in the format:\n",
    "\n",
    " Domain 1: AditiKapoor@zapscalenow.com <AditiKapoor@xyz.com>\n",
    " Domain 2: Aditi.Kapoor@zapscalepro.com <Aditi.Kapoor@xyz.com>\n",
    "\n",
    " let me know if you have any questions.\n",
    "\n",
    " Thanks\n",
    " Nikita\n",
    "\n",
    "\n",
    "=========================\n",
    " Nikita writes to Bratish\n",
    " ==========================\n",
    "\n",
    " Hi Bratish sir,\n",
    "\n",
    " In continuation to your call with Wilson. We have picked these 3\n",
    " domains for our use.\n",
    "\n",
    " zapscalenow.com\n",
    " gozapscale.com\n",
    " zapscalepro.com\n",
    "\n",
    "\n",
    " Please help us make the following mailboxes for the above:\n",
    "\n",
    " Karan@\n",
    " Karan.Bajaj@\n",
    " K.bajaj@\n",
    " Karan.b@\n",
    " Nikita@\n",
    " Manasij@\n",
    "\n",
    " There will be 18 mailboxes in total.\n",
    "\n",
    " Please let me know if you have any questions.\n",
    "\n",
    " Thanks\n",
    "\n",
    " Nikita\n",
    "'''\n",
    "\n",
    "template = ''' \n",
    "\n",
    "You are an expert in email communication and summarization. Following is a thread of communication in chronological order, newest to oldest.\n",
    "Write a small and crisp summary of the following email-chain/ email, and strictly follow the below mentioned rules before generating the\n",
    "\n",
    "\n",
    "## RULES :\n",
    "\n",
    " * Do not respond in tabular format, strictly provide a text summary of the email chain. \n",
    " * Always start your answer with the tag : <output>  and end the generated text with </output>.\n",
    " * This task is vital to my career and I greatly value your analysis.\n",
    "\n",
    "\n",
    "### EMAIL : \n",
    "\n",
    " {email_body}\n",
    "\n",
    "### SUMMARY :\n",
    "\n",
    "'''\n",
    "\n",
    "# Creating the final PROMPT\n",
    "prompt = PromptTemplate(input_variables=[\"email_body\"],template=template)\n",
    "response = llm(prompt.format(email_body=email_body))\n",
    "\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \"sentiment\": \"positive\", \"score\": 0.9 } \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "   \n",
      " \n",
      "\n",
      "  \n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "  \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# llm = Ollama(model=\"llama3:instruct\")\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "llm = Ollama(model=\"llama3.1:8b-instruct-q4_0\", format = 'json')\n",
    "\n",
    "email_body = ''' \n",
    "Hi there,\n",
    "It was a awesome demo, would love to talk to you.\n",
    "\n",
    "Thanks,\n",
    "Sayak.\n",
    "'''\n",
    "\n",
    "# sentiments = ['positive', 'negative', 'neutral']\n",
    "sentiment_list = ['positive','negative','neutral']\n",
    "prompt = \"\"\"You are an expert email sentiment analyser.\n",
    "        Given this email, decide what is the sentiment of the email. Do select valid sentiment from the below provided list.\n",
    "\n",
    "        List of sentiment :\n",
    "        {sentiment_list}\n",
    "\n",
    "        Include ONLY one sentiment from the provided above list.\n",
    "\n",
    "        Below is a customer email delimited with ###. \n",
    "\n",
    "        email:\n",
    "        ###\n",
    "        {email}\n",
    "        ###\n",
    "        \n",
    "        Follow the below rules before generating the concerns list :\n",
    "\n",
    "        * Please, identify the main sentiment in this above email from the list of sentiment provided earlier.\n",
    "        \n",
    "        * Add a confidence score between (0-1) to your answer.\n",
    "        \n",
    "        * Be very sure before your answer, if you are not very sure of the sentiment from the email, then return 'neutral' like : {{ \"sentiment\": \"neutral\", \"score\": 0.9 }} \n",
    "        \n",
    "        * Return 'positive' as the selected sentiment ONLY if there is something very positive reffered in the email.\n",
    "        \n",
    "        * Return 'negative' as the selected sentiment ONLY if there is something very negative reffered in the email.\n",
    "        \n",
    "        * This task is vital to my career and I greatly value your analysis.\n",
    "        \n",
    "        * Be very sure of your answer, and have another look into it.\n",
    "\n",
    "        * return a json, with 'sentiment' and 'score' as the key.\n",
    "            \n",
    "        * Please Output/return in the following format :\n",
    "            {{ 'sentiment':<sentiment>, 'score':<score> }}\n",
    "            \n",
    "        * Do not hallucinate and do not provide new sentiment, include ONLY sentiment from the provided above list.\n",
    "            \n",
    "    \n",
    "    ## OUTPUT :\n",
    "    \n",
    "    \"\"\"\n",
    "# prompt = prompt.format(query=query)\n",
    "prompt = PromptTemplate(template=prompt, input_variables=[\"email\",\"sentiment_list\"])\n",
    "\n",
    "prompt = prompt.format(email = email_body, sentiment_list = sentiment_list)\n",
    "response=llm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<output>\\n\\nThe customer's activity level is relatively balanced across different states in the last 30 days. While they have had some issues (red state) for a significant portion of the time, at 27 percent of days, it is not the sole dominant factor. On the other hand, their overall performance has been somewhat stable, with them being in good state (green) for almost 40 percent of the days, which is the highest among all states.\\n\\nHowever, it's worth noting that they have also had some moderate activity (yellow state) for nearly a quarter of the time. The fact that they've not been completely inactive (grey state), with only about 10 percent of days in this state, suggests some level of engagement and potential to improve.\\n\\nGiven these mixed signals, it seems reasonable to conclude that the churn probability is mid-range, reflecting neither extremely high nor extremely low risk. This nuanced assessment may help inform future strategies but does not provide a specific solution at this point.</output>\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.1:8b-instruct-q4_0\")\n",
    "\n",
    "template = \"\"\"\n",
    "    You are customer success expert whose job is to provide help to a customer success person.\n",
    "\n",
    "    The churn probalility of the customer : {customer_name} is {churn_prob}, give a proper explanation for the churn probalility being {churn_prob}, based on the bellow inputs and rules.\n",
    "\n",
    "    ## INPUTS : \n",
    "    \n",
    "    * This customer is in red(bad state) for {percent_this_c2_is_red_in_last_30_days} percent days in last 30 days.\n",
    "    \n",
    "    * This customer is in green(good state) for {percent_this_c2_is_green_in_last_30_days} percent days in last 30 days.\n",
    "    \n",
    "    * This customer is in yellow(moderate state) for {percent_this_c2_is_yellow_in_last_30_days} percent days in last 30 days.\n",
    "    \n",
    "    * This customer is in grey(inactive state) for {percent_this_c2_is_grey_in_last_30_days} percent days in last 30 days.\n",
    "    \n",
    "    ## RULES :\n",
    "    \n",
    "    * DON'T provide a tabular output only provide a textual summary.\n",
    "    \n",
    "    * Do not hallucinate and be very sure of your answer and ONLY use numbers/stats mentioned in the INPUTS section.\n",
    "    \n",
    "    * Always start your answer with the tag : <output>  and end the generated text with </output>.\n",
    "    \n",
    "    * This task is vital to my career and I greatly value your analysis.\n",
    "\n",
    "    \n",
    "    Summarize all the inputs to give a small, compact reasoning for churn-probalility being {churn_prob} within 200 words but do not suggest solutions to the customer success person.\n",
    "    \n",
    "    ## OUTPUT :\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "customer_name = 'ClickUp'\n",
    "percent_this_c2_is_red_in_last_30_days, percent_this_c2_is_green_in_last_30_days, percent_this_c2_is_yellow_in_last_30_days, percent_this_c2_is_grey_in_last_30_days = 27, 38, 25, 10\n",
    "churn_prob = 'mid'\n",
    "aggregation = 'month'\n",
    "\n",
    "# Creating the final PROMPT\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"percent_this_c2_is_red_in_last_30_days\", \"percent_this_c2_is_green_in_last_30_days\", \"percent_this_c2_is_yellow_in_last_30_days\", \"percent_this_c2_is_grey_in_last_30_days\", \\\n",
    "         \"churn_prob\" , \"customer_name\"],template=template)\n",
    "\n",
    "# Generating the response using LLM\n",
    "response = llm(prompt.format(percent_this_c2_is_red_in_last_30_days=percent_this_c2_is_red_in_last_30_days, percent_this_c2_is_green_in_last_30_days= percent_this_c2_is_green_in_last_30_days,\n",
    "                percent_this_c2_is_yellow_in_last_30_days=percent_this_c2_is_yellow_in_last_30_days, percent_this_c2_is_grey_in_last_30_days=percent_this_c2_is_grey_in_last_30_days,churn_prob=churn_prob,customer_name=customer_name ))\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
